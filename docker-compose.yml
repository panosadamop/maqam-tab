version: "3.9"

services:
  frontend:
    build:
      context: ./frontend
      dockerfile: ../docker/Dockerfile.frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
    networks:
      - maqamtab
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: ../docker/Dockerfile.backend
    ports:
      - "8000:8000"
    networks:
      - maqamtab
    environment:
      - PYTHONUNBUFFERED=1
      # Whisper model size: tiny | base | small | medium | large
      # larger = more accurate but slower & more RAM
      - WHISPER_MODEL=base
    volumes:
      # Cache Whisper models between container rebuilds
      - whisper_cache:/root/.cache/whisper
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 60s   # give Whisper model download time
      retries: 3

  # GPU variant (uncomment if you have NVIDIA GPU + nvidia-docker)
  # backend-gpu:
  #   extends: backend
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

networks:
  maqamtab:
    driver: bridge

volumes:
  whisper_cache:
